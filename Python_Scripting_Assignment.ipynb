{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Python Scripting :  Assignment**"
      ],
      "metadata": {
        "id": "iWx5YT-UFtFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. Write Python scripts for basic file operations and data processing?\n",
        "\n",
        "Sol -\n",
        "\n",
        "# file_manager.py\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def create_write_file():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    data = input(\"Enter text to write into file:\\n\")\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(data + \"\\n\")\n",
        "    print(f\"\\n File '{filename}' created and data written successfully!\\n\")\n",
        "\n",
        "def read_file():\n",
        "    filename = input(\"Enter file name to read: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            print(\"\\nFile Content:\\n\")\n",
        "            print(file.read())\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def append_file():\n",
        "    filename = input(\"Enter file name to append data: \")\n",
        "    data = input(\"Enter text to append:\\n\")\n",
        "    with open(filename, \"a\") as file:\n",
        "        file.write(data + \"\\n\")\n",
        "    print(f\"\\n Data appended to '{filename}' successfully!\\n\")\n",
        "\n",
        "def count_words_lines():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            lines = file.readlines()\n",
        "        num_lines = len(lines)\n",
        "        num_words = sum(len(line.split()) for line in lines)\n",
        "        print(f\"\\n Total Lines: {num_lines}\")\n",
        "        print(f\" Total Words: {num_words}\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def longest_word():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            words = file.read().split()\n",
        "        if words:\n",
        "            print(f\"\\n Longest Word: {max(words, key=len)}\\n\")\n",
        "        else:\n",
        "            print(\"\\n File is empty.\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def word_frequency():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            words = file.read().lower().split()\n",
        "        counts = Counter(words)\n",
        "        print(\"\\n Word Frequency:\\n\")\n",
        "        for word, count in counts.items():\n",
        "            print(f\"{word}: {count}\")\n",
        "        print()\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def remove_blank_lines():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            lines = file.readlines()\n",
        "        clean_lines = [line for line in lines if line.strip()]\n",
        "        with open(\"clean_\" + filename, \"w\") as new_file:\n",
        "            new_file.writelines(clean_lines)\n",
        "        print(f\"\\n Blank lines removed and saved to 'clean_{filename}'\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def uppercase_conversion():\n",
        "    filename = input(\"Enter file name: \")\n",
        "    try:\n",
        "        with open(filename, \"r\") as file:\n",
        "            data = file.read().upper()\n",
        "        with open(\"uppercase_\" + filename, \"w\") as new_file:\n",
        "            new_file.write(data)\n",
        "        print(f\"\\n File content converted to uppercase and saved as 'uppercase_{filename}'\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n File not found.\\n\")\n",
        "\n",
        "def copy_file():\n",
        "    source = input(\"Enter source file name: \")\n",
        "    destination = input(\"Enter destination file name: \")\n",
        "    try:\n",
        "        with open(source, \"r\") as src:\n",
        "            data = src.read()\n",
        "        with open(destination, \"w\") as dest:\n",
        "            dest.write(data)\n",
        "        print(f\"\\n File copied from '{source}' to '{destination}' successfully!\\n\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n Source file not found.\\n\")\n",
        "\n",
        "def menu():\n",
        "    while True:\n",
        "        print(\"=\"*50)\n",
        "        print(\"PYTHON FILE MANAGER & DATA PROCESSOR\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"1. Create & Write to File\")\n",
        "        print(\"2. Read File\")\n",
        "        print(\"3. Append to File\")\n",
        "        print(\"4. Copy File\")\n",
        "        print(\"5. Count Words & Lines\")\n",
        "        print(\"6. Find Longest Word\")\n",
        "        print(\"7. Word Frequency Count\")\n",
        "        print(\"8. Remove Blank Lines\")\n",
        "        print(\"9. Convert File to Uppercase\")\n",
        "        print(\"0. Exit\")\n",
        "        print(\"=\"*50)\n",
        "        choice = input(\"Enter your choice: \")\n",
        "\n",
        "        if choice == '1':\n",
        "            create_write_file()\n",
        "        elif choice == '2':\n",
        "            read_file()\n",
        "        elif choice == '3':\n",
        "            append_file()\n",
        "        elif choice == '4':\n",
        "            copy_file()\n",
        "        elif choice == '5':\n",
        "            count_words_lines()\n",
        "        elif choice == '6':\n",
        "            longest_word()\n",
        "        elif choice == '7':\n",
        "            word_frequency()\n",
        "        elif choice == '8':\n",
        "            remove_blank_lines()\n",
        "        elif choice == '9':\n",
        "            uppercase_conversion()\n",
        "        elif choice == '0':\n",
        "            print(\"\\nðŸ‘‹ Exiting program. Goodbye!\\n\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"\\n Invalid choice! Please try again.\\n\")\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    menu()\n",
        "\n",
        "\n",
        "\n",
        "Save this script as file_manager.py\n",
        "\n",
        "Open a terminal or command prompt in the scriptâ€™s directory\n",
        "\n",
        "python file_manager.py ---> run it\n"
      ],
      "metadata": {
        "id": "dKoFnQ1SFxI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Develop a simple web scraper to extract data from a website?\n",
        "\n",
        "Sol -\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# The URL of the target practice site\n",
        "URL = 'https://www.scrapethissite.com/pages/simple/'\n",
        "\n",
        "# List to hold the extracted data dictionaries\n",
        "scraped_data = []\n",
        "\n",
        "try:\n",
        "    # 1. Fetch the HTML Content\n",
        "    response = requests.get(URL)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "\n",
        "    # 2. Parse the HTML\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # 3. Target the relevant container (each country is in a div with class 'country')\n",
        "    country_containers = soup.find_all('div', class_='col-md-4 country')\n",
        "\n",
        "    # 4. Loop through each container and extract specific elements\n",
        "    for container in country_containers:\n",
        "        # Find the country name (h3 tag)\n",
        "        country_name_tag = container.find('h3', class_='country-name')\n",
        "        country = country_name_tag.text.strip() if country_name_tag else 'N/A'\n",
        "\n",
        "        # Find the capital (span tag)\n",
        "        capital_tag = container.find('span', class_='country-capital')\n",
        "        capital = capital_tag.text.strip() if capital_tag else 'N/A'\n",
        "\n",
        "        # Find the population (span tag)\n",
        "        population_tag = container.find('span', class_='country-population')\n",
        "        population = population_tag.text.strip() if population_tag else 'N/A'\n",
        "\n",
        "        # Find the area (span tag)\n",
        "        area_tag = container.find('span', class_='country-area')\n",
        "        area = area_tag.text.strip() if area_tag else 'N/A'\n",
        "\n",
        "        # Store the extracted data\n",
        "        scraped_data.append({\n",
        "            'Country': country,\n",
        "            'Capital': capital,\n",
        "            'Population': population,\n",
        "            'Area (kmÂ²)': area\n",
        "        })\n",
        "\n",
        "    # 5. Output the Data to a CSV file\n",
        "    if scraped_data:\n",
        "        filename = 'countries_data.csv'\n",
        "        keys = scraped_data[0].keys()\n",
        "\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=keys)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(scraped_data)\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"âœ… Successfully scraped {len(scraped_data)} countries.\")\n",
        "        print(f\"Data saved to {filename}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Optional: Print the first few results to the console\n",
        "    print(\"Sample Extracted Data:\")\n",
        "    for item in scraped_data[:5]:\n",
        "        print(item)\n",
        "\n",
        "except requests.exceptions.HTTPError as err:\n",
        "    print(f\"HTTP Error: {err}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "VjRZZM4UGTKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}